{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7096f6f1-4b21-435a-bd7e-1c3cbff0aa58",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bcdbfa-75e7-431d-9f12-1c7b5b0e8aef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read and randomize data\n",
    "import numpy as np\n",
    "import torch\n",
    "import sagemaker\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data = np.loadtxt(\"train.txt\", delimiter=',')\n",
    "perm_idx = np.random.permutation(data.shape[0])\n",
    "vali_num = int(data.shape[0] * 0.2)\n",
    "vali_idx = perm_idx[:vali_num]\n",
    "train_idx = perm_idx[vali_num:]\n",
    "\n",
    "# Split into training and validation data\n",
    "train_data = data[train_idx]\n",
    "vali_data = data[vali_idx]\n",
    "\n",
    "# Seperate features and labels\n",
    "train_features = train_data[:, 1:].astype(np.float32)\n",
    "train_labels = train_data[:, 0].astype(int)\n",
    "vali_features = vali_data[:, 1:].astype(np.float32)\n",
    "vali_labels = vali_data[:, 0].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a233d842-f3f3-495e-af04-fca63f2ba9c5",
   "metadata": {},
   "source": [
    "# Create NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa122798-5816-4373-89c8-cda8fd2985d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# define a Dataset class\n",
    "class HandNumDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx, :], self.labels[idx]\n",
    "\n",
    "\n",
    "train_data = HandNumDataset(train_features, train_labels)\n",
    "vali_data = HandNumDataset(vali_features, vali_labels)\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "vali_dataloader = DataLoader(vali_data, batch_size=batch_size)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d120c-6dbf-492c-9285-f73751b97a90",
   "metadata": {},
   "source": [
    "# Train model\n",
    "## Chosen parameters:\n",
    "Cross Entropy Loss Function, ReLU Activation Function, SGD with weight decay of 0.01, NN layers 768 -> 512 -> 1024 -> 512 -> 10, 7 Epoches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a073808-3fad-40b8-b7de-7eaa90b97c44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 5.612870  [   64/48000]\n",
      "loss: 0.432412  [ 6464/48000]\n",
      "loss: 0.278971  [12864/48000]\n",
      "loss: 0.117172  [19264/48000]\n",
      "loss: 0.181404  [25664/48000]\n",
      "loss: 0.213302  [32064/48000]\n",
      "loss: 0.073927  [38464/48000]\n",
      "loss: 0.205687  [44864/48000]\n",
      "Test Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.141408 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.194915  [   64/48000]\n",
      "loss: 0.152005  [ 6464/48000]\n",
      "loss: 0.160896  [12864/48000]\n",
      "loss: 0.049595  [19264/48000]\n",
      "loss: 0.050290  [25664/48000]\n",
      "loss: 0.126600  [32064/48000]\n",
      "loss: 0.033021  [38464/48000]\n",
      "loss: 0.149567  [44864/48000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.112933 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.067337  [   64/48000]\n",
      "loss: 0.088288  [ 6464/48000]\n",
      "loss: 0.062640  [12864/48000]\n",
      "loss: 0.006879  [19264/48000]\n",
      "loss: 0.013696  [25664/48000]\n",
      "loss: 0.093956  [32064/48000]\n",
      "loss: 0.012176  [38464/48000]\n",
      "loss: 0.065042  [44864/48000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.104731 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.034419  [   64/48000]\n",
      "loss: 0.039354  [ 6464/48000]\n",
      "loss: 0.024027  [12864/48000]\n",
      "loss: 0.005222  [19264/48000]\n",
      "loss: 0.011850  [25664/48000]\n",
      "loss: 0.052508  [32064/48000]\n",
      "loss: 0.012367  [38464/48000]\n",
      "loss: 0.034501  [44864/48000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.096842 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.015055  [   64/48000]\n",
      "loss: 0.010442  [ 6464/48000]\n",
      "loss: 0.020214  [12864/48000]\n",
      "loss: 0.001757  [19264/48000]\n",
      "loss: 0.005464  [25664/48000]\n",
      "loss: 0.012973  [32064/48000]\n",
      "loss: 0.008757  [38464/48000]\n",
      "loss: 0.009449  [44864/48000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.094335 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.009690  [   64/48000]\n",
      "loss: 0.003862  [ 6464/48000]\n",
      "loss: 0.014376  [12864/48000]\n",
      "loss: 0.001019  [19264/48000]\n",
      "loss: 0.005009  [25664/48000]\n",
      "loss: 0.003846  [32064/48000]\n",
      "loss: 0.002088  [38464/48000]\n",
      "loss: 0.013740  [44864/48000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.100618 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.003744  [   64/48000]\n",
      "loss: 0.002429  [ 6464/48000]\n",
      "loss: 0.001936  [12864/48000]\n",
      "loss: 0.000555  [19264/48000]\n",
      "loss: 0.001029  [25664/48000]\n",
      "loss: 0.002231  [32064/48000]\n",
      "loss: 0.001331  [38464/48000]\n",
      "loss: 0.017664  [44864/48000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.103477 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# loss function and optimizer\n",
    "lossFn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "# train data set\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, \"\n",
    "          f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "\n",
    "epochs = 7\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, lossFn, optimizer)\n",
    "    test(vali_dataloader, model, lossFn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907fa052-fb71-4839-b142-fcf7e76e3768",
   "metadata": {},
   "source": [
    "# Optimize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7f99b32-2009-4e1b-9c89-cd7ba69744e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Model 1\n",
    "# class NeuralNetwork1(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNetwork1, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(28*28, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# # Model 2\n",
    "# class NeuralNetwork2(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNetwork2, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(28*28, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 768),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(768, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# # Model 4\n",
    "# class NeuralNetwork3(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNetwork3, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(28*28, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "\n",
    "\n",
    "# # Model 4\n",
    "# class NeuralNetwork4(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(NeuralNetwork4, self).__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(28*28, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, 1024),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(1024, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 10)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "\n",
    "# epoches = [5, 7, 10, 13]\n",
    "# wDecay = [1e-1, 1e-2, 1e-3, 1e-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb218325-b514-47a5-932a-599b45f43ab0",
   "metadata": {},
   "source": [
    "## Convert to ONNX model from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20968f2-539b-4f30-a7d9-77db29a01498",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_features = np.loadtxt(\"test.txt\", delimiter=',')\n",
    "# print('array of testing feature matrix: shape ' + str(np.shape(test_features)))\n",
    "\n",
    "# model.eval()\n",
    "# x = torch.randn(batch_size, 1, 28, 28)\n",
    "# torch_out = torch_model(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # model_scripted = torch.jit.script(model)\n",
    "# # model_scripted.save(\"model.pt\")\n",
    "\n",
    "# # model1 = torch.jit.load(\"model.pt\")\n",
    "\n",
    "# # raw_pred = model1(torch.tensor(test_features).to(device).float())\n",
    "# # pred = np.argmax(raw_pred.to('cpu').detach().numpy(), axis=1)\n",
    "\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c18566-a6a3-4f70-89c6-b0bfa6de84f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NOTEBOOK_METADATA_FILE detected but failed to get valid domain and user from it.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-10-25-18-51-18-148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-25 18:51:18 Starting - Starting the training job...\n",
      "2023-10-25 18:51:35 Starting - Preparing the instances for training......\n",
      "2023-10-25 18:52:29 Downloading - Downloading input data...\n",
      "2023-10-25 18:53:05 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-25 18:53:18,306 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-25 18:53:18,308 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-10-25 18:53:18,317 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-25 18:53:18,319 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-25 18:53:18,484 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-10-25 18:53:18,495 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-10-25 18:53:18,506 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-10-25 18:53:18,515 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 64,\n",
      "        \"epochs\": 10,\n",
      "        \"learning-rate\": 0.01\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-10-25-18-51-18-148\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-981842671259/pytorch-training-2023-10-25-18-51-18-148/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"guesser\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"guesser.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":64,\"epochs\":10,\"learning-rate\":0.01}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=guesser.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=guesser\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-981842671259/pytorch-training-2023-10-25-18-51-18-148/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":64,\"epochs\":10,\"learning-rate\":0.01},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2023-10-25-18-51-18-148\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-981842671259/pytorch-training-2023-10-25-18-51-18-148/source/sourcedir.tar.gz\",\"module_name\":\"guesser\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"guesser.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"64\",\"--epochs\",\"10\",\"--learning-rate\",\"0.01\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.01\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 guesser.py --batch-size 64 --epochs 10 --learning-rate 0.01\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34m['train.txt']\u001b[0m\n",
      "\u001b[34mEpoch 1\u001b[0m\n",
      "\u001b[34m-------------------------------\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:38.938 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.095 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.096 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.096 algo-1:27 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.096 algo-1:27 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.096 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.098 algo-1:27 INFO hook.py:584] name:linear_relu_stack.0.weight count_params:401408\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.098 algo-1:27 INFO hook.py:584] name:linear_relu_stack.0.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.098 algo-1:27 INFO hook.py:584] name:linear_relu_stack.2.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.098 algo-1:27 INFO hook.py:584] name:linear_relu_stack.2.bias count_params:1024\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.098 algo-1:27 INFO hook.py:584] name:linear_relu_stack.4.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.098 algo-1:27 INFO hook.py:584] name:linear_relu_stack.4.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.098 algo-1:27 INFO hook.py:584] name:linear_relu_stack.6.weight count_params:5120\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.098 algo-1:27 INFO hook.py:584] name:linear_relu_stack.6.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.098 algo-1:27 INFO hook.py:586] Total Trainable Params: 1457162\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.098 algo-1:27 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2023-10-25 18:53:39.100 algo-1:27 INFO hook.py:476] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34mloss: 4.892719  [   64/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.301923  [ 6464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.215968  [12864/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.083977  [19264/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.346478  [25664/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.180259  [32064/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.074356  [38464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.124719  [44864/48000]\u001b[0m\n",
      "\u001b[34mTest Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.122716 \u001b[0m\n",
      "\u001b[34mEpoch 2\u001b[0m\n",
      "\u001b[34m-------------------------------\u001b[0m\n",
      "\u001b[34mloss: 0.016736  [   64/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.123843  [ 6464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.174853  [12864/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.042359  [19264/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.174952  [25664/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.050876  [32064/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.038513  [38464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.063358  [44864/48000]\u001b[0m\n",
      "\u001b[34mTest Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.098859 \u001b[0m\n",
      "\u001b[34mEpoch 3\u001b[0m\n",
      "\u001b[34m-------------------------------\u001b[0m\n",
      "\u001b[34mloss: 0.012357  [   64/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.035926  [ 6464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.090113  [12864/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.033525  [19264/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.066605  [25664/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.009749  [32064/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.030653  [38464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.037173  [44864/48000]\u001b[0m\n",
      "\u001b[34mTest Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.095696 \u001b[0m\n",
      "\u001b[34mEpoch 4\u001b[0m\n",
      "\u001b[34m-------------------------------\u001b[0m\n",
      "\u001b[34mloss: 0.012955  [   64/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.037453  [ 6464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.015329  [12864/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.035637  [19264/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.028743  [25664/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.007233  [32064/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.009349  [38464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.014643  [44864/48000]\u001b[0m\n",
      "\u001b[34mTest Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.093846 \u001b[0m\n",
      "\u001b[34mEpoch 5\u001b[0m\n",
      "\u001b[34m-------------------------------\u001b[0m\n",
      "\u001b[34mloss: 0.003644  [   64/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.007384  [ 6464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.004255  [12864/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.006481  [19264/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.026093  [25664/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.004064  [32064/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.006427  [38464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.024101  [44864/48000]\u001b[0m\n",
      "\u001b[34mTest Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.095133 \u001b[0m\n",
      "\u001b[34mEpoch 6\u001b[0m\n",
      "\u001b[34m-------------------------------\u001b[0m\n",
      "\u001b[34mloss: 0.006463  [   64/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.005821  [ 6464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.002230  [12864/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.003901  [19264/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.005146  [25664/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.002327  [32064/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.004134  [38464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.001373  [44864/48000]\u001b[0m\n",
      "\u001b[34mTest Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.094997 \u001b[0m\n",
      "\u001b[34mEpoch 7\u001b[0m\n",
      "\u001b[34m-------------------------------\u001b[0m\n",
      "\u001b[34mloss: 0.001304  [   64/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.003901  [ 6464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.001441  [12864/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.013906  [19264/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.006410  [25664/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.007212  [32064/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.001906  [38464/48000]\u001b[0m\n",
      "\u001b[34mloss: 0.001619  [44864/48000]\u001b[0m\n",
      "\u001b[34mTest Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.091616 \u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2023-10-25 18:54:01,944 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-10-25 18:54:16 Uploading - Uploading generated training model\n",
      "2023-10-25 18:54:16 Completed - Training job completed\n",
      "Training seconds: 107\n",
      "Billable seconds: 107\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "\n",
    "role = \"arn:aws:iam::981842671259:role/service-role/SageMaker-Piyush\"\n",
    "\n",
    "data_path = \"s3://holobolo-sagemaker-bucket/data/train.txt\"\n",
    "\n",
    "pytorch_estimator = PyTorch(entry_point=\"guesser.py\",\n",
    "                            instance_type=\"ml.c5.2xlarge\",\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version=\"1.8.0\",\n",
    "                            py_version=\"py3\",\n",
    "                            hyperparameters={\"epochs\": 10, \"batch-size\": 64,\n",
    "                                             \"learning-rate\": 1e-2})\n",
    "\n",
    "\n",
    "pytorch_estimator.fit({\"train\":\n",
    "                       \"s3://holobolo-sagemaker-bucket/data\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2a06f45-7276-4df7-95ad-7fd9204d1f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-east-1-981842671259/pytorch-training-2023-10-25-18-51-18-148/output/model.tar.gz), script artifact (s3://sagemaker-us-east-1-981842671259/pytorch-training-2023-10-25-18-51-18-148/source/sourcedir.tar.gz), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-east-1-981842671259/pytorch-training-2023-10-25-18-54-31-412/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-training-2023-10-25-18-54-31-412\n",
      "INFO:sagemaker:Creating endpoint-config with name pytorch-training-2023-10-25-18-54-31-412\n",
      "INFO:sagemaker:Creating endpoint with name pytorch-training-2023-10-25-18-54-31-412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = pytorch_estimator.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e638a36-1d76-4f7f-8905-78b466a36454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3.  18.\n",
      "   18.  18. 126. 136. 175.  26. 166. 255. 247. 127.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.  30.  36.  94. 154. 170. 253.\n",
      "  253. 253. 253. 253. 225. 172. 253. 242. 195.  64.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.  49. 238. 253. 253. 253. 253. 253.\n",
      "  253. 253. 253. 251.  93.  82.  82.  56.  39.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.  18. 219. 253. 253. 253. 253. 253.\n",
      "  198. 182. 247. 241.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.  80. 156. 107. 253. 253. 205.\n",
      "   11.   0.  43. 154.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.  14.   1. 154. 253.  90.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 139. 253. 190.\n",
      "    2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  11. 190. 253.\n",
      "   70.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  35. 241.\n",
      "  225. 160. 108.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  81.\n",
      "  240. 253. 253. 119.  25.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   45. 186. 253. 253. 150.  27.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.  16.  93. 252. 253. 187.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0. 249. 253. 249.  64.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   46. 130. 183. 253. 253. 207.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  39. 148.\n",
      "  229. 253. 253. 253. 250. 182.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  24. 114. 221. 253.\n",
      "  253. 253. 253. 201.  78.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.  23.  66. 213. 253. 253. 253.\n",
      "  253. 198.  81.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.  18. 171. 219. 253. 253. 253. 253. 195.\n",
      "   80.   9.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.  55. 172. 226. 253. 253. 253. 253. 244. 133.  11.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0. 136. 253. 253. 253. 212. 135. 132.  16.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"train.txt\", delimiter=',')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "test = data[0]\n",
    "test_features = test[1:]\n",
    "test_features = np.array([test_features])\n",
    "\n",
    "print(test_features)\n",
    "response = predictor.predict(list(test_features.astype(float)))\n",
    "pred = np.argmax(response)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef6e40-5ecf-464c-866b-543c08ff1391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
